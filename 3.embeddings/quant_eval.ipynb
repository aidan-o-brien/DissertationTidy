{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of Notebook\n",
    "\n",
    "I need to ensure that the document embeddings created are meaningful in some way. Therefore, it is important to evaluate them. To do this, I am going to create triplets of papers where two of them have an overlapping research area and third is unrelated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import load_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Papers and Research Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= r'C:\\Users\\aidan\\OneDrive - University of Bath\\1_Semester_2\\Cm50175_dissertation_preparation\\Data\\consolidate\\final_data'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv('papers.csv')\n",
    "areas = pd.read_csv('field_sources_list.csv')\n",
    "abstracts = pd.read_csv('abstracts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate Data\n",
    "\n",
    "Create a dataframe that consists of `EID | source id | list of asjc codes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed to evaluation\n",
    "drop_columns = ['Unnamed: 0', 'doi', 'title', 'afid', 'coverDate', 'publicationName', 'citedby_count']\n",
    "papers.drop(columns=drop_columns, inplace=True)\n",
    "papers.drop_duplicates(subset=['eid'], inplace=True)\n",
    "\n",
    "# Merge with abstracts to drop papers without abstracts\n",
    "merged = pd.merge(papers, abstracts, how='left', on='eid')\n",
    "merged.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "# Change format of areas such that each source_id corresponds to a list of asjc codes\n",
    "areas = areas.groupby(by=['source_id']).agg(lambda x: x.to_list())\n",
    "\n",
    "# Merge two dataframes\n",
    "merged = pd.merge(merged, areas, how='left', on='source_id')\n",
    "merged.drop(columns=['type', 'Unnamed: 0', 'doi', 'description'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to catch the fact that some papers will not have ASJC codes?__\n",
    "\n",
    "* Catch them in the function that generates the triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Triplets\n",
    "\n",
    "As a reminder, I want to create triplets whereby two papers are from the same research area and one paper from unrelated research areas. To start with, I will create 10,000 triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10,000 papers\n",
    "triplet_1 = merged[merged['asjc'].notnull()].sample(n=10000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop initial 10,000 papers\n",
    "merged.drop(triplet_1.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 out of 10,000 completed.\n",
      "2000 out of 10,000 completed.\n",
      "3000 out of 10,000 completed.\n",
      "4000 out of 10,000 completed.\n",
      "5000 out of 10,000 completed.\n",
      "6000 out of 10,000 completed.\n",
      "7000 out of 10,000 completed.\n",
      "8000 out of 10,000 completed.\n",
      "9000 out of 10,000 completed.\n",
      "10000 out of 10,000 completed.\n"
     ]
    }
   ],
   "source": [
    "# Find related papers\n",
    "def find_triplet(in_codes, df):\n",
    "    \n",
    "    # Sample merged\n",
    "    sample = df.sample(n=20000)\n",
    "    \n",
    "    match = None\n",
    "    match_codes = None\n",
    "    no_match = None\n",
    "    no_match_codes = None\n",
    "    \n",
    "    # Loop through samples until one matches\n",
    "    for i, row in sample.iterrows():\n",
    "        \n",
    "        # Convert codes to set\n",
    "        try:\n",
    "            if not row['asjc']:  # skip if no asjc codes present\n",
    "                continue\n",
    "            codes = set(row['asjc'])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        # Check if there is a match\n",
    "        if (in_codes & codes):\n",
    "            match = row['eid']\n",
    "            match_codes = codes\n",
    "        else:\n",
    "            no_match = row['eid']\n",
    "            no_match_codes = codes\n",
    "        \n",
    "        # Check stopping condition\n",
    "        if (match is not None) and (no_match is not None):\n",
    "            return {'match': match,\n",
    "                    'match_codes': match_codes,\n",
    "                    'no_match': no_match,\n",
    "                    'no_match_codes': no_match_codes}\n",
    "    \n",
    "    # No match\n",
    "    print('No match')\n",
    "    return None\n",
    "\n",
    "\n",
    "count = 0\n",
    "matches = []\n",
    "matches_codes = []\n",
    "no_matches = []\n",
    "no_matches_codes = []\n",
    "\n",
    "for index, in_row in triplet_1.iterrows():\n",
    "    try:\n",
    "        in_codes = set(in_row['asjc'])\n",
    "    except:\n",
    "        print(index)\n",
    "        print(row)\n",
    "        break\n",
    "    res = find_triplet(in_codes, merged)\n",
    "    \n",
    "    matches.append(res['match'])\n",
    "    matches_codes.append(res['match_codes'])\n",
    "    no_matches.append(res['no_match'])\n",
    "    no_matches_codes.append(res['no_match_codes'])\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f'{count} out of 10,000 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_1['matches'] = matches\n",
    "triplet_1['matches_codes'] = matches_codes\n",
    "triplet_1['no_matches'] = no_matches\n",
    "triplet_1['no_matches_codes'] = no_matches_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save triplets down just in case\n",
    "os.chdir(r'C:\\Users\\aidan\\OneDrive - University of Bath\\1_Semester_2\\Cm50175_dissertation_preparation\\Data\\consolidate\\embeddings')\n",
    "triplet_1.to_csv('triplets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Embeddings\n",
    "\n",
    "1. Load document embeddings\n",
    "2. For each triplet:\n",
    "    1. Find all three embeddings\n",
    "    2. Compute the cosine similarity between (eid, matches) and (eid, no_matches)\n",
    "    3. Add record (correct or incorrect) to `triplets` dataframe for further evaluation at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = pd.read_csv('triplets.csv')\n",
    "triplets.drop(columns=['source_id', 'asjc', 'matches_codes', 'no_matches_codes'], inplace=True)\n",
    "triplets.rename(columns={'Unnamed: 0': 'orig_idx'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load Embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model, vector_size, pretrained=False):\n",
    "    '''Function loads embeddings from respective directories and file formats.\n",
    "    model : word2vec, doc2vec, tfidf, glove, BERT\n",
    "    pretrained : True, False\n",
    "    vector_size : dimension of document embedding to load\n",
    "    '''\n",
    "    \n",
    "    # Change directory\n",
    "    fpath = r'C:\\Users\\aidan\\OneDrive - University of Bath\\1_Semester_2\\Cm50175_dissertation_preparation\\Data\\consolidate\\embeddings'\n",
    "    os.chdir(fpath + f'\\{model}')\n",
    "    \n",
    "    if model == 'word2vec':\n",
    "        if pretrained:\n",
    "            return np.load(f'word2vec_pretrained.npy')\n",
    "        else:\n",
    "            return np.load(f'my_w2v_{vector_size}_docvecs.npy')\n",
    "    elif model == 'doc2vec':\n",
    "        return np.load(f'my_d2v_{vector_size}.model.dv.vectors.npy')\n",
    "    elif model == 'tfidf':\n",
    "        return load_npz(f'tfidf_{vector_size}.npz').toarray()\n",
    "    elif model == 'glove':\n",
    "        return np.load(f'glove_pretrained_{vector_size}.npy').T\n",
    "    elif model == 'BERT':\n",
    "        return np.load('specter_embeddings.npy')\n",
    "    else:\n",
    "        print('Model not recognised! Please try again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "#d2v_100 = load_embeddings(model='doc2vec', vector_size=100, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluate Triplets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_triplets(triplets_df, model, vector_size, pretrained=False):\n",
    "    \n",
    "    # 1) Load document vectors\n",
    "    doc_vecs = load_embeddings(model=model, vector_size=vector_size, pretrained=pretrained)\n",
    "    print('Vectors loaded')\n",
    "    \n",
    "    # Create column in df to record correct / incorrect\n",
    "    if pretrained:\n",
    "        triplets_df[f'{model}_{vector_size}_pre'] = None\n",
    "    else:\n",
    "        triplets_df[f'{model}_{vector_size}'] = None\n",
    "    \n",
    "    # 2) Loop through each triplet\n",
    "    for i, row in tqdm(triplets_df.iterrows(), total=triplets_df.shape[0]):\n",
    "        \n",
    "        # Find index of the three articles - first already in df\n",
    "        #first_eid = row['eid']\n",
    "        second_eid = row['matches']\n",
    "        third_eid = row['no_matches']\n",
    "        \n",
    "        first_idx = row['orig_idx']\n",
    "        second_idx = merged[merged['eid'] == second_eid].index[0]\n",
    "        third_idx = merged[merged['eid'] == third_eid].index[0]\n",
    "        \n",
    "        # Retrieve document vectors for three articles\n",
    "        first_vec = doc_vecs[first_idx]\n",
    "        second_vec = doc_vecs[second_idx]\n",
    "        third_vec = doc_vecs[third_idx]\n",
    "        \n",
    "        # Compare three document vectors - cosine similarity\n",
    "        comp_one = np.dot(first_vec, second_vec) / (np.linalg.norm(first_vec) * np.linalg.norm(second_vec))\n",
    "        comp_two = np.dot(first_vec, third_vec) / (np.linalg.norm(first_vec) * np.linalg.norm(third_vec))\n",
    "        \n",
    "        # Update table - 'correct' is True or False\n",
    "        #res = {'first': comp_one,\n",
    "        #       'second': comp_two,\n",
    "        #       'correct': comp_one > comp_two}\n",
    "        res = comp_one > comp_two\n",
    "        if pretrained:\n",
    "            triplets_df.at[i, f'{model}_{vector_size}_pre'] = res\n",
    "        else:\n",
    "            triplets_df.at[i, f'{model}_{vector_size}'] = res\n",
    "    \n",
    "    return triplets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [26:16<00:00,  6.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# tfidf 10000\n",
    "test = evaluate_triplets(triplets, 'word2vec', 300, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc2vec_100</th>\n",
       "      <th>doc2vec_300</th>\n",
       "      <th>doc2vec_500</th>\n",
       "      <th>doc2vec_1000</th>\n",
       "      <th>glove_50</th>\n",
       "      <th>glove_100</th>\n",
       "      <th>glove_200</th>\n",
       "      <th>glove_300</th>\n",
       "      <th>word2vec_100</th>\n",
       "      <th>word2vec_300</th>\n",
       "      <th>word2vec_500</th>\n",
       "      <th>word2vec_1000</th>\n",
       "      <th>specter_768</th>\n",
       "      <th>tfidf_100</th>\n",
       "      <th>tfidf_300</th>\n",
       "      <th>tfidf_500</th>\n",
       "      <th>word2vec_300_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>6384</td>\n",
       "      <td>6122</td>\n",
       "      <td>6064</td>\n",
       "      <td>6036</td>\n",
       "      <td>7211</td>\n",
       "      <td>7157</td>\n",
       "      <td>7107</td>\n",
       "      <td>7134</td>\n",
       "      <td>7493</td>\n",
       "      <td>7556</td>\n",
       "      <td>7581</td>\n",
       "      <td>7581</td>\n",
       "      <td>7389</td>\n",
       "      <td>5166</td>\n",
       "      <td>5796</td>\n",
       "      <td>5973</td>\n",
       "      <td>7460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>3616</td>\n",
       "      <td>3878</td>\n",
       "      <td>3936</td>\n",
       "      <td>3964</td>\n",
       "      <td>2789</td>\n",
       "      <td>2843</td>\n",
       "      <td>2893</td>\n",
       "      <td>2866</td>\n",
       "      <td>2507</td>\n",
       "      <td>2444</td>\n",
       "      <td>2419</td>\n",
       "      <td>2419</td>\n",
       "      <td>2611</td>\n",
       "      <td>4834</td>\n",
       "      <td>4204</td>\n",
       "      <td>4027</td>\n",
       "      <td>2540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc2vec_100  doc2vec_300  doc2vec_500  doc2vec_1000  glove_50  \\\n",
       "True          6384         6122         6064          6036      7211   \n",
       "False         3616         3878         3936          3964      2789   \n",
       "\n",
       "       glove_100  glove_200  glove_300  word2vec_100  word2vec_300  \\\n",
       "True        7157       7107       7134          7493          7556   \n",
       "False       2843       2893       2866          2507          2444   \n",
       "\n",
       "       word2vec_500  word2vec_1000  specter_768  tfidf_100  tfidf_300  \\\n",
       "True           7581           7581         7389       5166       5796   \n",
       "False          2419           2419         2611       4834       4204   \n",
       "\n",
       "       tfidf_500  word2vec_300_pre  \n",
       "True        5973              7460  \n",
       "False       4027              2540  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.columns[4:]].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
