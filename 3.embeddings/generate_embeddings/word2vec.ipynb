{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.test import utils\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aidan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\aidan\\OneDrive - University of Bath\\1_Semester_2\\Cm50175_dissertation_preparation\\Data\\consolidate\\final_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 60267 rows due to missing abstracts.\n"
     ]
    }
   ],
   "source": [
    "abstracts_df = pd.read_csv('abstracts.csv')\n",
    "\n",
    "before = abstracts_df.shape[0]\n",
    "abstracts_df.dropna(subset=['description'], inplace=True)\n",
    "abstracts_df.reset_index(drop=True, inplace=True)\n",
    "after = abstracts_df.shape[0]\n",
    "\n",
    "print(f'Dropped {before - after} rows due to missing abstracts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "Unlike in doc2vec, the corpus has to be separated by sentences, not documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _, row in abstracts_df.iterrows():\n",
    "            for line in tokenize.sent_tokenize(row['description']):\n",
    "                words = utils.simple_preprocess(line)\n",
    "                words = [word for word in words if word not in stop_words]\n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 1000\n",
    "alpha = 0.025\n",
    "min_alpha = 0.0\n",
    "min_count = 2\n",
    "window_size = 5\n",
    "epochs = 10\n",
    "workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,\n",
    "                 vector_size=vector_size,\n",
    "                 alpha=alpha,\n",
    "                 min_alpha=min_alpha,\n",
    "                 min_count=min_count,\n",
    "                 epochs=epochs,\n",
    "                 workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model\n",
    "\n",
    "This automatically saves the model and word vectors as well.\n",
    "\n",
    "NOT ENOUGH MEMORY?? Is it saving the word vectors and not the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = r'C:\\Users\\aidan\\OneDrive - University of Bath\\1_Semester_2\\Cm50175_dissertation_preparation\\Data\\consolidate\\embeddings\\word2vec'\n",
    "os.chdir(fname)\n",
    "\n",
    "# Save keyed vectors! This was annoying to find out\n",
    "wv = model.wv\n",
    "wv.save(f'my_w2v_{vector_size}.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Document Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..\\\\embeddings\\\\word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDocCorpus:\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _, row in abstracts_df.iterrows():\n",
    "            words = utils.simple_preprocess(row['description'])\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "            yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = MyDocCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding(word2vec, doc):\n",
    "    # Remove OOV words\n",
    "    doc = [word for word in doc if word2vec.has_index_for(word)]\n",
    "    return np.mean(word2vec[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors loaded\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.72 GiB for an array with shape (1000, 767968) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-0e375d3c8bcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Vectors loaded'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnot_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mto_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.72 GiB for an array with shape (1000, 767968) and data type float64"
     ]
    }
   ],
   "source": [
    "num_docs = 767968\n",
    "vector_size = 1000\n",
    "wv = KeyedVectors.load(f'my_w2v_{vector_size}.kv')\n",
    "print('Vectors loaded')\n",
    "not_valid = 0\n",
    "to_save = np.empty((vector_size, num_docs))\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    try:\n",
    "        to_save[:,i] = get_doc_embedding(wv, doc)\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'my_w2v_{vector_size}_docvecs.npy', to_save.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable            Type                    Data/Info\n",
      "-----------------------------------------------------\n",
      "KeyedVectors        type                    <class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "MyDocCorpus         type                    <class '__main__.MyDocCorpus'>\n",
      "Word2Vec            type                    <class 'gensim.models.word2vec.Word2Vec'>\n",
      "abstracts_df        DataFrame                       Unnamed: 0       <...>[767968 rows x 4 columns]\n",
      "after               int                     767968\n",
      "before              int                     828235\n",
      "doc                 list                    n=152\n",
      "docs                MyDocCorpus             <__main__.MyDocCorpus obj<...>ct at 0x0000027C1390B550>\n",
      "download            method                  <bound method Downloader.<...>t at 0x0000027BAA4724E0>>\n",
      "get_doc_embedding   function                <function get_doc_embeddi<...>ng at 0x0000027BAC2A41E0>\n",
      "i                   int                     767967\n",
      "not_valid           int                     0\n",
      "np                  module                  <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "num_docs            int                     767968\n",
      "os                  module                  <module 'os' from 'C:\\\\Us<...>\\\\Anaconda3\\\\lib\\\\os.py'>\n",
      "pd                  module                  <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "smart_open          module                  <module 'smart_open' from<...>smart_open\\\\__init__.py'>\n",
      "stop_words          list                    n=179\n",
      "stopwords           WordListCorpusReader    <WordListCorpusReader in <...>ata\\\\corpora\\\\stopwords'>\n",
      "text                list                    n=66\n",
      "text_OOV_removed    list                    n=39\n",
      "tokenize            module                  <module 'nltk.tokenize' f<...>\\\\tokenize\\\\__init__.py'>\n",
      "utils               module                  <module 'gensim.test.util<...>\\gensim\\\\test\\\\utils.py'>\n",
      "vector_size         int                     1000\n",
      "word2vec            module                  <module 'gensim.models.wo<...>im\\\\models\\\\word2vec.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
