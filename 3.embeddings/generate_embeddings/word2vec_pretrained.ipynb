{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJkmZLmUj6hLCW14qIxC7j"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk9IX-_FKk-J"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE14aU8kKdki"
      },
      "source": [
        "import gensim.downloader as api\n",
        "from gensim import utils\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "from google.colab import drive\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LBYhttxPOMT",
        "outputId": "8d0db23a-6d22-4917-ce00-b391e7828237"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErPD-pM9Mx0_"
      },
      "source": [
        "# To Do\n",
        "\n",
        "1. Figure out how to average document embedding - DONE\n",
        "  * What do I do with out of vocabulary (OOV) words? - DONE\n",
        "    * It seems like you just get rid of the OOV words. FastText does attempt to deal with this.\n",
        "  * What do I do with words that appear more than once in document? - DONE\n",
        "    * Average them as if they were a separate term\n",
        "2. Write function to find document embedding - DONE\n",
        "3. Save document embedding vectors to .npy file - DONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYkrMTDLGyg"
      },
      "source": [
        "# Download Embeddings\n",
        "\n",
        "Word embeddings are 300 dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eZNEZDmLF-J"
      },
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvWwRCTyLf07"
      },
      "source": [
        "# Load and Pre-process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4yFW-XXQQ8n"
      },
      "source": [
        "os.chdir(\"/content/gdrive/MyDrive/Colab Notebooks/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1FAbMPKbNHL",
        "outputId": "cfe83f20-def9-4803-cd24-3ea3fc1102e7"
      },
      "source": [
        "download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0tkeILbdkx"
      },
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUAIJtHOLmZk"
      },
      "source": [
        "class MyCorpus:\n",
        "\n",
        "  def __iter__(self):\n",
        "    corpus_df = pd.read_csv('abstracts.csv')\n",
        "    corpus_df.dropna(subset=['description'], inplace=True)\n",
        "    corpus_df.reset_index(drop=True, inplace=True)\n",
        "    for _, row in corpus_df.iterrows():\n",
        "      words = utils.simple_preprocess(row['description'])\n",
        "      # Remove stop words\n",
        "      words = [word for word in words if word not in stop_words]\n",
        "      yield words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylvuiw82Rgaz"
      },
      "source": [
        "sentences = MyCorpus()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxRTJiMbUvmY"
      },
      "source": [
        "# Caculate Document Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHs2urwlU6kC"
      },
      "source": [
        "def get_doc_embedding(word2vec, doc):\n",
        "  '''This function calculates the document embedding from the relevant word\n",
        "  embeddings using word2vec trained on Google News.\n",
        "  Inputs : \n",
        "  - one document - list of words that make up that document\n",
        "  Returns :\n",
        "  - numpy array\n",
        "  '''\n",
        "\n",
        "  # Remove OOV words\n",
        "  doc = [word for word in doc if word in word2vec.vocab]\n",
        "\n",
        "  # Calculate mean\n",
        "  return np.mean(word2vec[doc], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtO6_SjTSLs1"
      },
      "source": [
        "test = utils.simple_preprocess('hello there, someone?')\n",
        "#get_doc_embedding(wv, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPYQuOLSTB8m"
      },
      "source": [
        "num_docs = 767968\n",
        "num_dims = 300\n",
        "not_valid = []\n",
        "to_save = np.empty((num_dims, num_docs))\n",
        "for i, doc in enumerate(sentences):\n",
        "  try:\n",
        "    to_save[:,i] = get_doc_embedding(wv, doc)\n",
        "  except:\n",
        "    not_valid.append(i)\n",
        "    to_save[:,i] = np.zeros(num_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPAgKjl-fpUf"
      },
      "source": [
        "np.save('word2vec_pretrained.npy', to_save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cg9uJPPZ83p"
      },
      "source": [
        "np.save('word2vec_pretrained.npy', vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAw9gKMzVOUv"
      },
      "source": [
        "# Check Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WASO92au7qOh"
      },
      "source": [
        "vecs = np.load('word2vec_pretrained.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt-XqU8mYIbL",
        "outputId": "4278d84a-2531-433f-d3f4-fe526ac73f7c"
      },
      "source": [
        "vecs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(767968, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_3t9jvdVqmJ",
        "outputId": "38468585-ba3e-4178-e58c-c3d157d8b7c1"
      },
      "source": [
        "num_zeros = 0\n",
        "\n",
        "for vec in vecs:\n",
        "  if np.all(vec == 0):\n",
        "    num_zeros += 1\n",
        "\n",
        "print(num_zeros)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLjqYv1DYTO8"
      },
      "source": [
        "This means that there was 1 paper which did not have any words in the pretrained word2vec's vocabulary. I'm happy to keep this in as it will only affect 1 or a few comparisons out of 10,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hquS0iewW_6O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
